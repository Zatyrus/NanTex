{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dependencies\n",
    "# Model\n",
    "from NanTex_backend.deep_learning.dl_building_blocks import *\n",
    "from NanTex_backend.deep_learning.dl_model_assembly import assembled_model, hyperparameters\n",
    "\n",
    "# Dataloaders\n",
    "from NanTex_backend.batching.BatchDataLoader import BatchDataLoader_Handler\n",
    "from NanTex_backend.data_preparation import img_augmentation as img_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup image augmentation\n",
    "train_augmentation_pipeline = img_augmentation.train_augmentation_pipeline\n",
    "val_augmentation_pipeline = img_augmentation.val_augmentation_pipeline\n",
    "\n",
    "## Setup BatchDataLoader\n",
    "BDL:BatchDataLoader_Handler\n",
    "BDL = BatchDataLoader_Handler.from_config(config_file_path=None, DEBUG=False)\n",
    " \n",
    "BDL.setup_augmentation_pipelines(raw_aug = train_augmentation_pipeline, \n",
    "                                 val_aug = val_augmentation_pipeline)\n",
    "\n",
    "train_gen, val_gen = BDL.build_BatchDataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check if CUDA is available\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'num_epochs': 100,\n",
       " 'steps_per_epoch': 10,\n",
       " 'learning_rate': 0.0005,\n",
       " 'weight_decay': 0,\n",
       " 'num_workers': 4,\n",
       " 'pin_memory': True,\n",
       " 'shuffle': True,\n",
       " 'seed': None,\n",
       " 'log_interval': 10,\n",
       " 'save_interval': 10,\n",
       " 'save_dir': './model'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Hyperparams\n",
    "hyperparameters.update({\"save_dir\":\"./model\"})\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model', 'activation', 'loss_fn', 'optimizer', 'device', 'writer']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Assembled model\n",
    "list(assembled_model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e006e47cb2c4c6780809904ab0a5bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current batch loss: 0:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Model Setup Test\n",
    "train(train_loader = train_gen,\n",
    "      val_loader = val_gen,\n",
    "      net = assembled_model['model'],\n",
    "      activation = assembled_model['activation'],\n",
    "      optimizer = assembled_model['optimizer'],\n",
    "      loss_fn= assembled_model['loss_fn'],\n",
    "      dtype= torch.FloatTensor,\n",
    "      writer = assembled_model['writer'],\n",
    "      device = assembled_model['device'],\n",
    "      training_steps = hyperparameters['num_epochs'],\n",
    "      log_interval = hyperparameters['log_interval'],\n",
    "      save_interval = hyperparameters['save_interval'],\n",
    "      save_dir = hyperparameters['save_dir'],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now for some actually useful hyperparameters\n",
    "-> to be continued"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NanTex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
